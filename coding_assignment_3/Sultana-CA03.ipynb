{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848b2c47",
   "metadata": {},
   "source": [
    "# Necessary import for getting directory and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fff7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import html\n",
    "import feedparser\n",
    "import re\n",
    "import textacy\n",
    "import spacy\n",
    "import textacy.preprocessing as tprep\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c135e0",
   "metadata": {},
   "source": [
    "# Use BeautifulSoup to extract, for each news article, the title/headline, the publication time, and the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a282178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## URL from where all the news articles need to be downloaded\n",
    "feed_reuters = feedparser.parse(\n",
    "    'http://web.archive.org/web/20200613003232if_/http://feeds.reuters.com/Reuters/worldNews')\n",
    "\n",
    "## directory in where all the html files are going to be piled up\n",
    "CA_DIR = os.getcwd() + \"/Sultana-CA03\" \n",
    "\n",
    "info = [] #empty list for saving title, publication time and content of each news article\n",
    "\n",
    "for url in feed_reuters.entries[:]:\n",
    "    file_name = url.id.split(\"/\")[-1] + \".html\"\n",
    "    html = requests.get(url.id)\n",
    "    path = os.path.join(CA_DIR, file_name)\n",
    "    \n",
    "    with open(path, \"w+\") as f: ### w+ opens a file for both writing and reading\n",
    "        f.write(html.text) \n",
    "        f.close()\n",
    "    soup = BeautifulSoup(html.text, 'html.parser') ### read in an HTML file and work on it\n",
    "    \n",
    "    for title in soup.find_all('title'):\n",
    "        tag_info = title.get_text() #extract title name\n",
    "        \n",
    "        ptime = soup.find(\"meta\", { 'property': \"og:article:published_time\"})['content'] #extract publication time\n",
    "            \n",
    "        text = soup.get_text() #extract text\n",
    "        \n",
    "        details = {\"Title\":tag_info,\n",
    "                    \"Pucblication Time\":ptime,\n",
    "                  \"Content\":text}\n",
    "        info.append(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f1b07",
   "metadata": {},
   "source": [
    "# Store the extracted information into a Pandas DataFrame so that each news article is an entry and the columns include Title, Time, Content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77afdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title     Pucblication Time  \\\n",
      "0   Mexico City to begin gradual exit from lockdow...  2020-06-13T00:25:34Z   \n",
      "1   Mexico reports record tally of 5,222 new coron...  2020-06-13T00:16:17Z   \n",
      "2   Venezuela top court names new electoral counci...  2020-06-12T23:38:23Z   \n",
      "3   One-fifth of Britain's coronavirus patients we...  2020-06-12T23:02:17Z   \n",
      "4   France to lift border controls for EU travelle...  2020-06-12T22:58:37Z   \n",
      "5   Brazil's COVID-19 deaths surge past UK, WHO sa...  2020-06-12T20:03:58Z   \n",
      "6   Canada's Trudeau calls arrest video of indigen...  2020-06-12T19:26:13Z   \n",
      "7   Egypt registers highest daily rise in coronavi...  2020-06-12T22:10:31Z   \n",
      "8   Artists around the world pay tribute to George...  2020-06-12T21:45:49Z   \n",
      "9   Brazil's COVID-19 death toll passes Britain, w...  2020-06-12T21:56:06Z   \n",
      "10  Lebanon protesters burn roads, clash with secu...  2020-06-12T21:13:32Z   \n",
      "11  'Stop buying social peace at our expense', Fre...  2020-06-12T08:44:34Z   \n",
      "12  Soccer team mobs coffin of murdered Mexican te...  2020-06-12T21:10:22Z   \n",
      "13  Chile's military moves through shadows to spot...  2020-06-12T21:03:32Z   \n",
      "14  Somalia's Islamist group al Shabaab says sets ...  2020-06-12T20:39:05Z   \n",
      "15  Congo's gold being smuggled out by the tonne, ...  2020-06-12T20:31:57Z   \n",
      "16  Brazil drops police from human rights report a...  2020-06-12T20:17:58Z   \n",
      "17  Slovenia to open borders for Italians, Montene...  2020-06-12T20:27:06Z   \n",
      "18  Botswana reinstates strict coronavirus lockdow...  2020-06-12T20:25:44Z   \n",
      "19  Coronavirus hitting the Americas hardest says ...  2020-06-12T16:30:30Z   \n",
      "\n",
      "                                              Content  \n",
      "0   Mexico City to begin gradual exit from lockdow...  \n",
      "1   Mexico reports record tally of 5,222 new coron...  \n",
      "2   Venezuela top court names new electoral counci...  \n",
      "3   One-fifth of Britain's coronavirus patients we...  \n",
      "4   France to lift border controls for EU travelle...  \n",
      "5   Brazil's COVID-19 deaths surge past UK, WHO sa...  \n",
      "6   Canada's Trudeau calls arrest video of indigen...  \n",
      "7   Egypt registers highest daily rise in coronavi...  \n",
      "8   Artists around the world pay tribute to George...  \n",
      "9   Brazil's COVID-19 death toll passes Britain, w...  \n",
      "10  Lebanon protesters burn roads, clash with secu...  \n",
      "11  'Stop buying social peace at our expense', Fre...  \n",
      "12  Soccer team mobs coffin of murdered Mexican te...  \n",
      "13  Chile's military moves through shadows to spot...  \n",
      "14  Somalia's Islamist group al Shabaab says sets ...  \n",
      "15  Congo's gold being smuggled out by the tonne, ...  \n",
      "16  Brazil drops police from human rights report a...  \n",
      "17  Slovenia to open borders for Italians, Montene...  \n",
      "18  Botswana reinstates strict coronavirus lockdow...  \n",
      "19  Coronavirus hitting the Americas hardest says ...  \n"
     ]
    }
   ],
   "source": [
    "data_raw= pd.DataFrame (info)\n",
    "print (data_raw) #print the dataframe with raw contents\n",
    "\n",
    "## Raw DF is then converted to a .csv file for the convenience of further analysis\n",
    "data_raw.to_csv(\"./data_raw.csv\") \n",
    "root_dir = os.getcwd()\n",
    "file_dir = root_dir + \"/data_raw.csv\"\n",
    "df = pd.read_csv(file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90602c3",
   "metadata": {},
   "source": [
    "# On the above dataset, cleanup the data using the clean and normalize functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa76a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if textacy.__version__ < '0.11':\n",
    "    def normalize(text):\n",
    "        text = tprep.normalize_hyphenated_words(text)\n",
    "        text = tprep.normalize_quotation_marks(text)\n",
    "        text = tprep.normalize_unicode(text)\n",
    "        text = tprep.remove_accents(text)\n",
    "        return text\n",
    "else:\n",
    "    # adjusted to textacy 0.11. Note, function names are changed\n",
    "    def normalize(text):\n",
    "        text = tprep.normalize.hyphenated_words(text)\n",
    "        text = tprep.normalize.quotation_marks(text)\n",
    "        text = tprep.normalize.unicode(text)\n",
    "        text = tprep.remove.accents(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87acea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Standard cleaning function\n",
    "def clean(text):\n",
    "    # convert html escapes like & to characters.\n",
    "    try: \n",
    "        text = html.unescape(text) \n",
    "    except AttributeError:\n",
    "        print(\"Attribute error: ignored plz\")\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # removing mentions \n",
    "    text = re.sub(\"@\\S+\", \"\", text)\n",
    "    text = re.sub('[-%!@#$]', '', text)\n",
    "    text = re.sub(\"@[A-Za-z0-9]+\",\"\",text)\n",
    "    #Removing numerical data\n",
    "    text = re.sub(r'\\d+','',text)\n",
    "    #Removing currencies \n",
    "    text = re.sub(r'[\\$\\d+\\d+\\$]','',text)\n",
    "    #Handling all date formats\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    #Removing a hyperlink\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '',text)\n",
    "    #Extracting the main domain name of a URL\n",
    "#     text = re.search(r'[\\.\\/]+(.*)\\.',text)\n",
    "#     #Removing all punctuation\n",
    "    text = re.sub(r'[^a-z0-9A-Z_]',' ',text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b54067e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n",
      "Attribute error: ignored plz\n"
     ]
    }
   ],
   "source": [
    "df[\"Content\"] = df[\"Content\"].map(lambda x: clean(x))       #Clean the Content column\n",
    "df[\"Content\"] = df[\"Content\"].map(lambda x: normalize(x))   #Normalise the Content column\n",
    "\n",
    "##Store the contents in a list named 'txt' for further processing\n",
    "txt = []\n",
    "for row in df[\"Content\"]:\n",
    "    txt.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0b660",
   "metadata": {},
   "source": [
    "# for each article, lemmas, nounds, noun-phrases, and entity pairs. Combine them into a dictionary and save it to a file in a format of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9104133",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") ##English pipeline optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660dcbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert each article content into spacy defined pipeline\n",
    "doc = []\n",
    "for j in range(0,len(txt)):\n",
    "    d = nlp(txt[j])\n",
    "    doc.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2fdeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function for generating Lemma, POS, Noun_Phrase and Entity Pairs for each article and \n",
    "##save them in a dict called 'row' then this dict is converted into dataframe df_cln\n",
    "\n",
    "def display_nlp(doc, include_punct=False):\n",
    "    \"\"\"Generate data frame for visualization of spaCy tokens.\"\"\"\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct or include_punct:\n",
    "            row = {'Content':doc,\n",
    "                   'token':i,\n",
    "                   'text': t.text, 'lemma_': t.lemma_, \n",
    "                   'Part of Speech': t.pos_, \"Noun Phrase\":{chunk.text for chunk in doc.noun_chunks}, \n",
    "                   \"Entity Pairs\":{ent.text for ent in doc.ents}}\n",
    "            rows.append(row)\n",
    "    \n",
    "    df_cln = pd.DataFrame(rows).set_index('token')\n",
    "    df_cln.index.name = \"Index\"\n",
    "    return df_cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880d389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pass each content into the above function and save it into a .csv file. For overwrite mode = 'a' is chosen\n",
    "for t in range (len(doc)):\n",
    "    df_up = display_nlp(doc[t])\n",
    "    df_up.to_csv('./data_category.csv',mode='a',index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e0106f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Visualize the .csv file\n",
    "categorized_file = root_dir + \"/data_category.csv\" #file_location\n",
    "df_segmented = pd.read_csv(categorized_file) #read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90586b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>Part of Speech</th>\n",
       "      <th>Noun Phrase</th>\n",
       "      <th>Entity Pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mexico City to begin gradual exit from lockdow...</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>{'gradual exit', 'Friday', 'very orderly trans...</td>\n",
       "      <td>{'Thursday', 'Friday', 'minutes', 'Raul Cortes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mexico City to begin gradual exit from lockdow...</td>\n",
       "      <td>City</td>\n",
       "      <td>City</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>{'gradual exit', 'Friday', 'very orderly trans...</td>\n",
       "      <td>{'Thursday', 'Friday', 'minutes', 'Raul Cortes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mexico City to begin gradual exit from lockdow...</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "      <td>{'gradual exit', 'Friday', 'very orderly trans...</td>\n",
       "      <td>{'Thursday', 'Friday', 'minutes', 'Raul Cortes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mexico City to begin gradual exit from lockdow...</td>\n",
       "      <td>begin</td>\n",
       "      <td>begin</td>\n",
       "      <td>VERB</td>\n",
       "      <td>{'gradual exit', 'Friday', 'very orderly trans...</td>\n",
       "      <td>{'Thursday', 'Friday', 'minutes', 'Raul Cortes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mexico City to begin gradual exit from lockdow...</td>\n",
       "      <td>gradual</td>\n",
       "      <td>gradual</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>{'gradual exit', 'Friday', 'very orderly trans...</td>\n",
       "      <td>{'Thursday', 'Friday', 'minutes', 'Raul Cortes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content     text   lemma_  \\\n",
       "0  Mexico City to begin gradual exit from lockdow...   Mexico   Mexico   \n",
       "1  Mexico City to begin gradual exit from lockdow...     City     City   \n",
       "2  Mexico City to begin gradual exit from lockdow...       to       to   \n",
       "3  Mexico City to begin gradual exit from lockdow...    begin    begin   \n",
       "4  Mexico City to begin gradual exit from lockdow...  gradual  gradual   \n",
       "\n",
       "  Part of Speech                                        Noun Phrase  \\\n",
       "0          PROPN  {'gradual exit', 'Friday', 'very orderly trans...   \n",
       "1          PROPN  {'gradual exit', 'Friday', 'very orderly trans...   \n",
       "2           PART  {'gradual exit', 'Friday', 'very orderly trans...   \n",
       "3           VERB  {'gradual exit', 'Friday', 'very orderly trans...   \n",
       "4            ADJ  {'gradual exit', 'Friday', 'very orderly trans...   \n",
       "\n",
       "                                        Entity Pairs  \n",
       "0  {'Thursday', 'Friday', 'minutes', 'Raul Cortes...  \n",
       "1  {'Thursday', 'Friday', 'minutes', 'Raul Cortes...  \n",
       "2  {'Thursday', 'Friday', 'minutes', 'Raul Cortes...  \n",
       "3  {'Thursday', 'Friday', 'minutes', 'Raul Cortes...  \n",
       "4  {'Thursday', 'Friday', 'minutes', 'Raul Cortes...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_segmented.head(5) #Show top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c40d36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>Part of Speech</th>\n",
       "      <th>Noun Phrase</th>\n",
       "      <th>Entity Pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8281</th>\n",
       "      <td>Coronavirus hitting the Americas hardest says ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "      <td>{'exchanges', 'other parts', 'the global hotsp...</td>\n",
       "      <td>{'Americas', 'Friday', 'minutes', 'four', 'Nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8282</th>\n",
       "      <td>Coronavirus hitting the Americas hardest says ...</td>\n",
       "      <td>All</td>\n",
       "      <td>all</td>\n",
       "      <td>DET</td>\n",
       "      <td>{'exchanges', 'other parts', 'the global hotsp...</td>\n",
       "      <td>{'Americas', 'Friday', 'minutes', 'four', 'Nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>Coronavirus hitting the Americas hardest says ...</td>\n",
       "      <td>Rights</td>\n",
       "      <td>Rights</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>{'exchanges', 'other parts', 'the global hotsp...</td>\n",
       "      <td>{'Americas', 'Friday', 'minutes', 'four', 'Nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8284</th>\n",
       "      <td>Coronavirus hitting the Americas hardest says ...</td>\n",
       "      <td>Reserved</td>\n",
       "      <td>Reserved</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>{'exchanges', 'other parts', 'the global hotsp...</td>\n",
       "      <td>{'Americas', 'Friday', 'minutes', 'four', 'Nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8285</th>\n",
       "      <td>Coronavirus hitting the Americas hardest says ...</td>\n",
       "      <td>forphoneonlyfortabletportraitupfortabletlandsc...</td>\n",
       "      <td>forphoneonlyfortabletportraitupfortabletlandsc...</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>{'exchanges', 'other parts', 'the global hotsp...</td>\n",
       "      <td>{'Americas', 'Friday', 'minutes', 'four', 'Nor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Content  \\\n",
       "8281  Coronavirus hitting the Americas hardest says ...   \n",
       "8282  Coronavirus hitting the Americas hardest says ...   \n",
       "8283  Coronavirus hitting the Americas hardest says ...   \n",
       "8284  Coronavirus hitting the Americas hardest says ...   \n",
       "8285  Coronavirus hitting the Americas hardest says ...   \n",
       "\n",
       "                                                   text  \\\n",
       "8281                                                      \n",
       "8282                                                All   \n",
       "8283                                             Rights   \n",
       "8284                                           Reserved   \n",
       "8285  forphoneonlyfortabletportraitupfortabletlandsc...   \n",
       "\n",
       "                                                 lemma_ Part of Speech  \\\n",
       "8281                                                             SPACE   \n",
       "8282                                                all            DET   \n",
       "8283                                             Rights          PROPN   \n",
       "8284                                           Reserved          PROPN   \n",
       "8285  forphoneonlyfortabletportraitupfortabletlandsc...           NOUN   \n",
       "\n",
       "                                            Noun Phrase  \\\n",
       "8281  {'exchanges', 'other parts', 'the global hotsp...   \n",
       "8282  {'exchanges', 'other parts', 'the global hotsp...   \n",
       "8283  {'exchanges', 'other parts', 'the global hotsp...   \n",
       "8284  {'exchanges', 'other parts', 'the global hotsp...   \n",
       "8285  {'exchanges', 'other parts', 'the global hotsp...   \n",
       "\n",
       "                                           Entity Pairs  \n",
       "8281  {'Americas', 'Friday', 'minutes', 'four', 'Nor...  \n",
       "8282  {'Americas', 'Friday', 'minutes', 'four', 'Nor...  \n",
       "8283  {'Americas', 'Friday', 'minutes', 'four', 'Nor...  \n",
       "8284  {'Americas', 'Friday', 'minutes', 'four', 'Nor...  \n",
       "8285  {'Americas', 'Friday', 'minutes', 'four', 'Nor...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_segmented.tail(5) #show last 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a3dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

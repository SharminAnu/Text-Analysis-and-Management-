Two distributions, p and q are close to each other if their KL-divergence is close to 0. DL-divergence of p to q over a set of outcomes is defined by   .
1.	Compute the distribution of classes in the training data on the "Priority" label obtained from df in TextClassification.ipynb. Is this distribution close to that of df?
2.	Compute the distrubiton of classes in the training data on the "Priority" label obtained from df_balanced in TextClassfication.ipynb. Is this distribution close to that of df_new? 
3.	What conclusions can you draw about the training set selections against the underlying datasets from the label-distribution point of view?
